# Set Seer

https://duncanwood.github.io/set-seer/

<!-- predictable-intro-start -->
In my family, there was one game to rule them all: **SET**. 

There's something beautiful in the simplicity and honesty of the ruleset. 
But there's one inevitable moment where a judgment call is required: 
sometimes there really is no SET in play. 

To acknowledge this feels like giving up. 
Maybe we're just not good enough. 

Set Seer is here to help. 
<!-- predictable-intro-end -->

## Usage

Set Seer is designed to be a quick, definitive judge during gameplay. It operates in two modes:

### 1. Default Mode (The Judge)
When you first open the app, it acts as an impartial observer. Point your camera at the cards on the table.
-   **What you see**: Green bounding boxes around every detected card. No reading required.
-   **The Verdict**: Look at the button in the top bar.
    -   **Green "Show Sets"**: A set exists! The group can keep searching with the reassurance that a solution is possible.
    -   **Red "No Sets!"**: No sets are currently visible. You can confidently deal more cards.

### 2. Show Sets Mode (The Spoiler)
If you've given up or just want to see the solution, tap the **Green "Show Sets"** button.
-   **What you see**: The green boxes disappear. Instead, specific sets are highlighted with distinct colors.
-   **How it works**:
    -   Cards belonging to the same set share a unique border color (e.g., all cards in Set A are red, all in Set B are blue).
    -   If a single card belongs to multiple valid sets, it will have multiple concentric borders.

### 3. Debug Mode
Tap the **Wrench icon** in the bottom right corner to enable Debug Mode.
-   **What you see**: Detailed card labels (Color, Shape, Count, Shading), confidence percentages, and performance stats (Inference Time).
-   **Use case**: Helpful for understanding why the AI might be misidentifying a card.

## Technical Details

### How it Works
The application runs a lightweight computer vision model directly in your web browser. No images are ever sent to a server; everything is processed locally on your device for speed and privacy.
-   **Frontend**: Next.js (React)
-   **Inference**: ONNX Runtime Web
-   **Platform**: Optimized for mobile web browsers

### The Model: Zero-Shot Real World Performance
The core of Set Seer is a YOLOv8 object detection model. What makes this model unique is its training strategy: **it has seen exactly zero real-world photos of Set cards during training.**

#### Synthetic Data Pipeline
Getting a large, labeled dataset of Set cards in various lighting conditions and angles is tedious. Instead, I built a robust synthetic data generation pipeline (`model-dev/set_seer_dataset.py`) that creates training data on the fly.

Every single training image is generated by:
1.  **Backgrounds**: Random textures from the Describable Textures Dataset (DTD) to prevent overfitting to table surfaces.
2.  **Card Assets**: High-quality digital assets of the 81 unique Set cards.
3.  **Augmentation**:
    -   **Perspective Transforms**: Warping cards to simulate viewing angles.
    -   **Occlusion**: Randomly covering parts of cards to make the model robust to partial visibility.
    -   **Color Jittering**: Varying brightness, contrast, and saturation to handle different lighting environments.
    -   **Scale & Rotation**: Randomly sizing and rotating cards across the canvas.

This "domain randomization" approach forces the model to learn the fundamental features of the cards (shape, color, shading, count) rather than memorizing specific lighting conditions or camera artifacts.

#### Training Infrastructure
The model was trained using **Google Colab GPUs**. While TPUs are often faster, the Ultralytics YOLO implementation currently has better compatibility and support on GPU runtimes. The training script (`model-dev/train_custom.py`) handles the custom data loading and validation loop.

## Running the Web App

To run the Set Seer web application locally:

```bash
cd web-app
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

## Developing the Model

If you want to experiment with the model training:

1.  Navigate to the `model-dev` directory.
    ```bash
    cd model-dev
    ```
2.  Install dependencies (requires PyTorch and Ultralytics).
    ```bash
    pip install -r requirements.txt # (if available) or install manually
    ```
3.  Run the training script (this will start generating synthetic data and training).
    ```bash
    python train_custom.py --epochs 10 --imgsz 640
    ```
